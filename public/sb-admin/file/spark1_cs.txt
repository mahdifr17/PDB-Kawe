from __future__ import print_function

import sys

import numpy as np
from pyspark import SparkContext
from pyspark.mllib.clustering import KMeans


def parseVector(line):
    return np.array([float(x) for x in line.split(',')])


if __name__ == "__main__":
    sc = SparkContext(appName="KMeans")
    lines = sc.textFile("output_positions_v2.csv")
    data = lines.map(parseVector)
    k = 48

    model = KMeans.train(data, k)
    print("Final centers: " + str(model.clusterCenters))
#    print("Count : " + str(model.summary.clusterSize))
#    print("Total data: "+str(model.clusterSizes))
#    print("Total Cost: " + str(model.computeCost(data)))
    sc.stop()
